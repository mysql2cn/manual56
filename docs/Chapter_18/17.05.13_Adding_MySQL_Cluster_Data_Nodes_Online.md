# 17.5.13. 在线添加MySQL集群数据节点 ###

### 17.5.13. 在线添加MySQL集群数据节点 ###

此章节描述了如何在线添加MySQL集群数据节点，既是，不需要去完全关闭MySQL集群并且重启进程。
>[重点]   
>目前，你必须添加新的数据节点作为一个新的MySQL集群节点组。此外，它是不可能在线改变副本的数量（或每个节点组节点的数量）。

#### 17.5.13.1. 在线添加MySQL集群数据节点：一般问题 ####


本小节提供一些当在线添加MySQL集群节点的时候一些当前行为限制信息。   

**数据再分配.**      在线添加新节点的能力包含重组NDBCLUSTER表数据和索引,因此她们被分布在所有的数据节点上,包括这最新的节点，在内存和磁盘数据表都支持表重组。这个再分配当前不支持唯一索引(仅仅排好序的索引是可以被再分配)或者blob表数据。   

在新的数据节点没有被自动添加之前，NDBCLUSTER[md]表的再分配已经存在生效了，但是在mysql或者其他MySQL客户端应用使用简单的sql语句就可以实现。尽管如此，所有添加到表的已经创建的数据和索引，在新的节点组成功添加完成后，自动在所有的集群数据节点之间自动分配，包括已经作为节点组一部分的新添加的节点组。   

**部分启动.**    所有新的数据节点组没有启动，也可以添加新的节点组。也可以在低版本的集群中添加一个新的节点组--既是，集群可以部分启动，或者一个或者多个数据节点没有启动。在接下来的例子中，在新的节点组被添加进来之前，集群必须拥有足够的可用的正在运行的节点。   

**对正在运行中的操作的影响.**   使用MySQL集群数据的DML操作不会被新的节点组的创建或者添加阻止，或者被表的重组阻止。然而，在当前表重构的时候不可能执行DDL操作--既是，当一条ALTER TABLE ... REORGANIZE PARTITION[md]正在执行的时候，任何其他的DDL操作不能被执行。此外，在ALTER TABLE ... REORGANIZE PARTITION[md]执行期间(或者任何其他DDL语句执行)，也不可以去重启集群数据节点。   


**故障处理.**   在节点组创建和表重组期间数据节点故障处理，如下表所述：   

<table cellspacing="0px" border="1"><colgroup><col><col class="2"><col><col class="4"></colgroup><thead><tr><th scope="col" rowspan="2">故障发生时期:</th><th scope="col" colspan="3">故障发生在:</th></tr><tr><th scope="col"><span class="quote">“<span class="quote">旧</span>”</span> 数据节点</th><th scope="col"><span class="quote">“<span class="quote">新</span>”</span> 数据节点 </th><th scope="col">系统</th></tr></thead><tbody><tr><td scope="row">创建节点组</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果非master节点故障: </b>节点组的创建总是会向前滚动。                        
</p></li><li class="listitem"><p><b>如果master节点故障： </b></p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><b>如果内部提交点已经完成: </b>节点组的创建是向前滚动。
                          </p></li><li class="listitem"><p><b>如果内部提交点没有完成： </b>节点组的创建就会回滚。
</p></li></ul>
</div>
</li></ul>
</div>
</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果非master节点故障: </b>节点组创建总是继续往前滚动。
</p></li><li class="listitem"><p><b>如果master故障: </b></p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><b>如果内部提交点已经完成：</b>节点组创建继续往前滚动.
                          </p></li><li class="listitem"><p><b>如果内部提交点没有完成. </b>节点组创建需要回滚。
</p></li></ul>
</div>
</li></ul>
</div>
</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果<code class="literal">CREATE NODEGROUP</code>的执行已经完成了内部提交点: </b>当重启时，集群包含了新的节点组，否则就没有。                       
                      </p></li><li class="listitem"><p><b>如果 <code class="literal">CREATE NODEGROUP</code> 的执行没有完成内部提交点: </b>当重启时，集群就不会包含新的节点组。
</p></li></ul>
</div>
</td></tr><tr><td scope="row">表重组</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果非master节点故障: </b>表重组总是继续往前滚动。
</p></li><li class="listitem"><p><b>如果master节点故障: </b></p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><b>如果内部提交点已经完成: </b>表重组继续往前滚动。
                          </p></li><li class="listitem"><p><b>如果内部提交点没有完成。 </b>表重组需要回滚。
</p></li></ul>
</div>
</li></ul>
</div>
</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果非master节点故障: </b>表重组总是往前继续滚动。
</p></li><li class="listitem"><p><b>如果master节点故障: </b></p>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: circle; "><li class="listitem"><p><b>如果内部提交点已经完成: </b>表重组继续往前滚动。
                          </p></li><li class="listitem"><p><b>如果内部提交点没有完成。 </b>
                            表重组需要回滚。
</p></li></ul>
</div>
</li></ul>
</div>
</td><td>
<div class="itemizedlist">
<ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><b>如果<code class="literal">ALTER ONLINE TABLE
                        <em class="replaceable"><code>table</code></em> REORGANIZE
                        PARTITION</code> 语句已经完成了内部提交点: </b>
当集群重启，数据以及索引将属于被<span class="quote">“<span class="quote">new</span>”</span>数据节点分配的表。
                       
                       
                      </p></li><li class="listitem"><p><b>如果<code class="literal">ALTER ONLINE TABLE
                        <em class="replaceable"><code>table</code></em> REORGANIZE
                        PARTITION</code> 语句没有完成内部提交点: </b>
                        当集群重启时，数据和索引属于被<span class="quote">“<span class="quote">old</span>”</span>数据节点分配的表。
</p></li></ul>
</div>
</td></tr></tbody></table>

**删除节点组.**   ndb_mgm[md]客户端支持DROP NODEGROUP命令，但是仅仅当节点组的数据节点没有包含任何数据时，才可以去删除节点组。因为当前没有办法去清空一个特定的数据节点或者节点组，仅仅在如下2个条件下这个命令才能执行：    

1 . 在ndb\_mgm[md]客户端执行CREATE NODEGROUP完之后，而且必须在mysql[md]客户端执行ALTER ONLINE TABLE ... REORGANIZE PARTITION[md]语句之前。   

2 . 在使用DROP TABLE[md]删除所有NDBCLUSTER[md]表之后。   
TRUNCATE TABLE[md]不能达到此目的，因为数据节点继续保留了表定义。   

#### 17.5.13.2. 在线添加MySQL集群数据节点：基本过程 ####

在此章节，我们列出了往MySQL集群里添加新的数据节点所需要的基本步骤。这个过程决定了为了数据节点进程你是否正在使用ndbd或者ndbmtd[md]二进制日志。更加细节的例子	，参见17.5.13.3章节, “Adding MySQL Cluster Data Nodes Online: Detailed Example”[md]。   

假如你已经有了一个正在运行的MySQL集群，在线添加数据节点需要以下步骤：   
1 . 编辑集群配置文件config.ini，添加新的与添加的节点相关的[ndbd]段落。在此例中，集群使用了多个管理服务器，这些变更在多个管理器服务器的所有config.ini文件里面使用。

你必须很小心，在config.ini文件新添加的任何数据节点不能与现有的节点的节点ID重叠。在事件中，你必须使用动态分配节点ID的API节点，并且这些ID与跟你想去使用新数据节点的节点ID匹配，可以强迫任何类似的API节点迁移，就像接下来此过程描述的那样。   

2 . 执行MySQL集群管理服务器的滚动重启。
>[重点]   
>所有的管理服务器必须使用--reload [2342] 或者 --initial [2341][md]选项去强制读取新的配置文件。   

3 . 执行已经存在的MySQL集群数据节点的滚动重启。当重启已经存在的数据节点没有必要(或者通常甚至是可取的)使用--initial [2330][md]选项。   

如果你正在使用动态分配节点ID的API节点，并且这些ID与跟你想去使用新数据节点的节点ID匹配，在此步骤中重启任意数据节点之前你必须重启完所有的API节点(包含SQL节点)。这会导致以前没有明确指派给节点ID的任意API节点放弃这些节点ID并获得新的节点ID。   

4 . 执行任何连接到MySQL集群的SQL或者API节点的滚动重启。   
5 . 启动新的数据节点。   
   新的数据节点可以以任何的顺序重启。他们也可以同时重启，如同他们也可以在已经存在的数据节点滚动重启完成之后启动，并且必须在执行下一个步骤之前。   
6 . 在MySQL集群管理客户端执行一个或者多个CREATE NODEGROUP命令去创建新的节点组或者新的数据节点所属的节点组。   
7 . 对于每一个NDBCLUSTER[md]表在mysql[md]客户端执行ALTER ONLINE TABLE ... REORGANIZE PARTITION语句，在所有数据节点之间重新分布集群的数据。   
>[注意]   
>这个仅仅需要在已经存在同时新添加的表上完成。在添加的新节点组被自动分配后，表里的数据创建完成。然而，在新的节点添加之前， 不会使用新的节点把添加到任意指定的存在的在的表tb的数据给分配掉， 直到使用ALTER ONLINE TABLE tbl REORGANIZE PARTITION[md]命令后这个表被重组。   

8 . 对于每个NDBCLUSTER表，在mysql客户端执行OPTIMIZE TABLE命令，从旧的节点上回收空闲空间。   

你能添加所有需要的节点，然后使用一些CREATE NODEGROUP命令成功把节点添加到集群里。

#### 17.5.13.3. 在线添加MySQL集群数据节点：详细案例 ####

在此小节，我们提供一个详细的案例描述如何在线添加MySQL集群数据节点，启动一个在单节点组里有2个数据节点的MySQL集群，并且包含一个集群拥有2个节点组4个数据节点。   

**启动配置.**  阐述的目的，我们假定一个最低的配置，并且集群使用包含如下信息的config.ini文件：   

	[ndbd default]
	DataMemory = 100M
	IndexMemory = 100M
	NoOfReplicas = 2
	DataDir = /usr/local/mysql/var/mysql-cluster
	
	[ndbd]
	Id = 1
	HostName = 192.168.0.1
	
	[ndbd]
	Id = 2
	HostName = 192.168.0.2
	
	[mgm]
	HostName = 192.168.0.10
	Id = 10
	
	[api]
	Id=20
	HostName = 192.168.0.20
	
	[api]
	Id=21
	HostName = 192.168.0.21


>[注意]    
>在数据节点ID和其他节点之间的序列中我们留下一个空隙，这将使得给新建立的数据节点分配节点ID变得容易一些。

我们也假定你已经使用了合适的命令行或者在my.cnf选项里面启动了集群，并且在管理器客户端运行show命令，会输入如下所示类似的报告：   

	-- NDB Cluster -- Management Client --
	ndb_mgm> SHOW
	Connected to Management Server at: 192.168.0.10:1186
	Cluster Configuration
	---------------------
	[ndbd(NDB)]     2 node(s)
	id=1    @192.168.0.1  (5.6.14-ndb-7.3.4, Nodegroup: 0, *)
	id=2    @192.168.0.2  (5.6.14-ndb-7.3.4, Nodegroup: 0)
	
	[ndb_mgmd(MGM)] 1 node(s)
	id=10   @192.168.0.10  (5.6.14-ndb-7.3.4)
	
	[mysqld(API)]   2 node(s)
	id=20   @192.168.0.20  (5.6.14-ndb-7.3.4)
	id=21   @192.168.0.21  (5.6.14-ndb-7.3.4)

最后我们假定集群已经包含了一个单一的创建好的NDBCLUSTER[md]表，如下所示：  

	USE n;
	
	CREATE TABLE ips (
	    id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
	    country_code CHAR(2) NOT NULL,
	    type CHAR(4) NOT NULL,
	    ip_address varchar(15) NOT NULL,
	    addresses BIGINT UNSIGNED DEFAULT NULL,
	    date BIGINT UNSIGNED DEFAULT NULL
	)   ENGINE NDBCLUSTER;

在往这个表添加大约50000条记录之后，内存使用以及在关联信息将会显示出来。   
>[注意]   
>在此列中，我们展示单一线程ndbd[md]被使用于数据节点进程，然而，从MySQL集群NDB 7.0.4开始--你也可以适用此例，对于ndbd，如果你正在使用多线程ndbmtd取代ndbmtd[md]，无论他是否出现在后续的步骤中。(Bug #43108)   

**步骤1: 修伽配置文件.**
用文本编辑器打开集群全局配置文件，并且添加相关2个数据节点的[ndbd]小段(我指定数据节点ID为3和4，并且假定他们能分别在192.168.0.3和192.168.0.4运行起来)。在你添加完新的小段之后，config.ini配置文件的应该看起来如下所示，以粗体显示部分为主： 
  
	[ndbd default]
	DataMemory = 100M
	IndexMemory = 100M
	NoOfReplicas = 2
	DataDir = /usr/local/mysql/var/mysql-cluster
	
	[ndbd]
	Id = 1
	HostName = 192.168.0.1
	
	[ndbd]
	Id = 2
	HostName = 192.168.0.2
	
	[ndbd]
	Id = 3
	HostName = 192.168.0.3
	
	[ndbd]
	Id = 4
	HostName = 192.168.0.4
	
	[mgm]
	HostName = 192.168.0.10
	Id = 10
	
	[api]
	Id=20
	HostName = 192.168.0.20
	
	[api]
	Id=21
	HostName = 192.168.0.21

一旦做好了必须的变更，就保存文件。   

**步骤2：重启管理服务器.**  重启集群管理服务器需要你分别执行命令关闭管理服务器然后启动管理服务器，如下所示：  
1 . 在管理器客户端执行stop命令停止管理服务器，如下所示： 

	ndb_mgm> 10 STOP
	Node 10 has shut down.
	Disconnecting to allow Management Server to shutdown

	shell>

2 . 由于关闭管理服务器会引起管理客户端终止，你必须从系统shell上面重新启动管理器客户端，简单来说，我们假定config.ini文件与管理服务器二进制日志目录处于同一个目录,但是实际上,你必须对配置文件应用对应的工作,你必须适用--reload [2342]或者--initial [2341]选项,相对于配置缓存,因此管理服务器从文件读取了新的配置。如果shell命令的当前目录是管理服务器二进制所处的目录，然后你可以如下所示调用管理服务器：   

	shell> ndb_mgmd -f config.ini --reload
	2008-12-08 17:29:23 [MgmSrvr] INFO     -- NDB Cluster Management Server. 5.6.14-ndb-7.3.4
	2008-12-08 17:29:23 [MgmSrvr] INFO     -- Reading cluster configuration from 'config.ini'


在重启ndb_mgm[md]进程后，你可以在管理器客户端检查show的命令输出，你现在应该可以看到如下的信息：  

	-- NDB Cluster -- Management Client --
	ndb_mgm> SHOW
	Connected to Management Server at: 192.168.0.10:1186
	Cluster Configuration
	---------------------
	[ndbd(NDB)]     2 node(s)
	id=1    @192.168.0.1  (5.6.14-ndb-7.3.4, Nodegroup: 0, *)
	id=2    @192.168.0.2  (5.6.14-ndb-7.3.4, Nodegroup: 0)
	id=3 (not connected, accepting connect from 192.168.0.3)
	id=4 (not connected, accepting connect from 192.168.0.4)
	
	[ndb_mgmd(MGM)] 1 node(s)
	id=10   @192.168.0.10  (5.6.14-ndb-7.3.4)
	
	[mysqld(API)]   2 node(s)
	id=20   @192.168.0.20  (5.6.14-ndb-7.3.4)
	id=21   @192.168.0.21  (5.6.14-ndb-7.3.4)

**步骤3： 对已经存在的数据节点执行滚动重启.**   这个可以全部在管理服务器客户端执行RESTART命令来完成，如下所示： 

	ndb_mgm> 1 RESTART
	Node 1: Node shutdown initiated
	Node 1: Node shutdown completed, restarting, no start.
	Node 1 is being restarted
	
	ndb_mgm> Node 1: Start initiated (version 7.3.4)
	Node 1: Started (version 7.1.30)
	
	ndb_mgm> 2 RESTART
	Node 2: Node shutdown initiated
	Node 2: Node shutdown completed, restarting, no start.
	Node 2 is being restarted
	
	ndb_mgm> Node 2: Start initiated (version 7.3.4)
	
	ndb_mgm> Node 2: Started (version 7.3.4)

>[重点]   
>在每一个执行X RESTART命令之后，在执行其他更进一步操作之前，等待直道管理器客户端报告出Node X: Started (version ...)。



**步骤 4：对所有的API节点执行滚动重启.**   对随后的mysqld_safe执行mysqladmin shutdown命令在集群中的SQL节点，关闭并且重启每一个MySQL服务器(或者其他启动脚本)。与如下所述很类似，对于指定的MySQL服务实例需要指定root帐号密码。   

	shell> mysqladmin -uroot -ppassword shutdown
	081208 20:19:56 mysqld_safe mysqld from pid file
	/usr/local/mysql/var/tonfisk.pid ended
	shell> mysqld_safe --ndbcluster --ndb-connectstring=192.168.0.10 &
	081208 20:20:06 mysqld_safe Logging to '/usr/local/mysql/var/tonfisk.err'.
	081208 20:20:06 mysqld_safe Starting mysqld daemon with databases
	from /usr/local/mysql/var

当然，确切的输入输出依赖于在系统里面在何处以何种方式安装MySQL，如同你选择启动MySQL的方式一样(这些选项的部分或者全部都可以在my.cnf文件里面指定)。   

**步骤 5: 新数据节点执行初始化启动.** 对于所有的新数据节点，从每一个主机的系统shell上，如下所示启动数据节点，使用--initial [2330]选项：  

	shell> ndbd -c 192.168.0.10 --initial

>[注意]   
>不像重启已经存在的数据节点，你可以同时重启新的数据节点；不需要等待这个节点启动完成之后再去启动另外一个节点。   

在执行下一个步骤之前，等待直道所有的新的节点已经重启完成。一旦新的数据节点已经重启完成，你可以在管理客户端使用SHOW命令看到它们并不属于任意节点组的输出信息(如同粗体部分所示)：

	ndb_mgm> SHOW
	Connected to Management Server at: 192.168.0.10:1186
	Cluster Configuration
	---------------------
	[ndbd(NDB)]     2 node(s)
	id=1    @192.168.0.1  (5.6.14-ndb-7.3.4, Nodegroup: 0, *)
	id=2    @192.168.0.2  (5.6.14-ndb-7.3.4, Nodegroup: 0)
	id=3    @192.168.0.3  (5.6.14-ndb-7.3.4, no nodegroup) [ct]
	id=4    @192.168.0.4  (5.6.14-ndb-7.3.4, no nodegroup) [ct]
	
	[ndb_mgmd(MGM)] 1 node(s)
	id=10   @192.168.0.10  (5.6.14-ndb-7.3.4)
	
	[mysqld(API)]   2 node(s)
	id=20   @192.168.0.20  (5.6.14-ndb-7.3.4)
	id=21   @192.168.0.21  (5.6.14-ndb-7.3.4)

**步骤 6 ：创建一个新的节点组.** 在集群管理客户端通过执行CREATE NODEGROUP命令来实现，此命令需要作为它的参数以逗号分隔的包含新数据节点组的数据节点的节点ID列表，如下所示：   

	ndb_mgm> CREATE NODEGROUP 3,4
	Nodegroup 1 created
   
通过再次执行SHOW，你可以验证出数据节点3以及4以及加入到新的节点组(再次以粗体标示)：  

	ndb_mgm> SHOW
	Connected to Management Server at: 192.168.0.10:1186
	Cluster Configuration
	---------------------
	[ndbd(NDB)]     2 node(s)
	id=1    @192.168.0.1  (5.6.14-ndb-7.3.4, Nodegroup: 0, *)
	id=2    @192.168.0.2  (5.6.14-ndb-7.3.4, Nodegroup: 0)
	id=3    @192.168.0.3  (5.6.14-ndb-7.3.4, Nodegroup: 1)
	id=4    @192.168.0.4  (5.6.14-ndb-7.3.4, Nodegroup: 1)
	
	[ndb_mgmd(MGM)] 1 node(s)
	id=10   @192.168.0.10  (5.6.14-ndb-7.3.4)
	
	[mysqld(API)]   2 node(s)
	id=20   @192.168.0.20  (5.6.14-ndb-7.3.4)
	id=21   @192.168.0.21  (5.6.14-ndb-7.3.4)
  

**步骤 7：集群数据再分配.**     当节点组创建时，已经存在的数据和索引不会自动分布到新的节点组的数据节点上面，你可以在管理客户端通过执行适当的REPORT命令看到：  

	ndb_mgm> ALL REPORT MEMORY
	
	Node 1: Data usage is 5%(177 32K pages of total 3200)
	Node 1: Index usage is 0%(108 8K pages of total 12832)
	Node 2: Data usage is 5%(177 32K pages of total 3200)
	Node 2: Index usage is 0%(108 8K pages of total 12832)
	Node 3: Data usage is 0%(0 32K pages of total 3200)
	Node 3: Index usage is 0%(0 8K pages of total 12832)
	Node 4: Data usage is 0%(0 32K pages of total 3200)
	Node 4: Index usage is 0%(0 8K pages of total 12832)

用-p选项使用ndb_desc命令，会在输出中产生分区信息，你可以看到表仅仅只使用了2个分区(在输出的每一个分区信息段落中，如下粗体所示)：   

	shell> ndb_desc -c 192.168.0.10 -d n ips -p
	-- ips --
	Version: 1
	Fragment type: 9
	K Value: 6
	Min load factor: 78
	Max load factor: 80
	Temporary table: no
	Number of attributes: 6
	Number of primary keys: 1
	Length of frm data: 340
	Row Checksum: 1
	Row GCI: 1
	SingleUserMode: 0
	ForceVarPart: 1
	FragmentCount: 2
	TableStatus: Retrieved
	-- Attributes --
	id Bigint PRIMARY KEY DISTRIBUTION KEY AT=FIXED ST=MEMORY AUTO_INCR
	country_code Char(2;latin1_swedish_ci) NOT NULL AT=FIXED ST=MEMORY
	type Char(4;latin1_swedish_ci) NOT NULL AT=FIXED ST=MEMORY
	ip_address Varchar(15;latin1_swedish_ci) NOT NULL AT=SHORT_VAR ST=MEMORY
	addresses Bigunsigned NULL AT=FIXED ST=MEMORY
	date Bigunsigned NULL AT=FIXED ST=MEMORY
	
	-- Indexes --
	PRIMARY KEY(id) - UniqueHashIndex
	PRIMARY(id) - OrderedIndex
	
	-- Per partition info --
	Partition   Row count   Commit count  Frag fixed memory   Frag varsized memory
	0           26086       26086         1572864             557056
	1           26329       26329         1605632             557056
	
	NDBT_ProgramExit: 0 - OK


通过对每一个NDBCLUSTER[md]表，在mysql客户端执行ALTER ONLINE TABLE ... REORGANIZE PARTITION statement[md]语句，能所有的数据节点中引发数据重新分配。在执行ALTER ONLINE TABLE ips REORGANIZE PARTITION[md]语句后，你可以使用ndb_desc[md]看到这个表的数据已经存储到了4个分区中，如下所示(报告中的粗体相关部分)：   


	shell> ndb_desc -c 192.168.0.10 -d n ips -p
	-- ips --
	Version: 16777217
	Fragment type: 9
	K Value: 6
	Min load factor: 78
	Max load factor: 80
	Temporary table: no
	Number of attributes: 6
	Number of primary keys: 1
	Length of frm data: 341
	Row Checksum: 1
	Row GCI: 1
	SingleUserMode: 0
	ForceVarPart: 1
	FragmentCount: 4
	TableStatus: Retrieved
	-- Attributes --
	id Bigint PRIMARY KEY DISTRIBUTION KEY AT=FIXED ST=MEMORY AUTO_INCR
	country_code Char(2;latin1_swedish_ci) NOT NULL AT=FIXED ST=MEMORY
	type Char(4;latin1_swedish_ci) NOT NULL AT=FIXED ST=MEMORY
	ip_address Varchar(15;latin1_swedish_ci) NOT NULL AT=SHORT_VAR ST=MEMORY
	addresses Bigunsigned NULL AT=FIXED ST=MEMORY
	date Bigunsigned NULL AT=FIXED ST=MEMORY
	
	-- Indexes --
	PRIMARY KEY(id) - UniqueHashIndex
	PRIMARY(id) - OrderedIndex
	
	-- Per partition info --
	Partition   Row count   Commit count  Frag fixed memory   Frag varsized memory
	0           12981       52296         1572864             557056
	1           13236       52515         1605632             557056
	2           13105       13105         819200              294912
	3           13093       13093         819200              294912
	
	NDBT_ProgramExit: 0 - OK

>[注意]     
>通常来讲，ALTER [ONLINE] TABLE table_name REORGANIZE PARTITION[md]使用分区列表标示和分区定义集合为已经明确分区过的表重新定义新的分区图表，就此而言在这里使用的往新的MySQL集群节点组重新分配数据是一个例外；当使用这个方法，仅仅表名字被用于下述表关键字，并且在接下来的REORGANIZE PARTITION中没有别的关键字。   
>关于更多的信息，请参见13.1.7章节，“ALTER TABLE Syntax”[md]。   

此外，对于每一个表，ALTER ONLINE TABLE[md]语句应该跟在OPTIMIZE TABLE[md]之后回收废弃的空间。你能获取所有NDBCLUSTER表的列表，通过对INFORMATION_SCHEMA.TABLES表使用下述查询：  

	SELECT TABLE_SCHEMA, TABLE_NAME
	    FROM INFORMATION_SCHEMA.TABLES
	    WHERE ENGINE = 'NDBCLUSTER';


>[注意]：  
>MySQL集群表的INFORMATION_SCHEMA.TABLES.ENGINE值总是NDBCLUSTER [md]，跟用来创建表的CREATE TABLE语句在此的ENGINE选项里是否使用NDB或者NDBCLUSTER无关。   


执行完这些语句之后在ALL REPORT MEMORY的输出中，你能看到数据和索引在所有的数据节点之间被重新分配，如下所示：   

	ndb_mgm> ALL REPORT MEMORY
	
	Node 1: Data usage is 5%(176 32K pages of total 3200)
	Node 1: Index usage is 0%(76 8K pages of total 12832)
	Node 2: Data usage is 5%(176 32K pages of total 3200)
	Node 2: Index usage is 0%(76 8K pages of total 12832)
	Node 3: Data usage is 2%(80 32K pages of total 3200)
	Node 3: Index usage is 0%(51 8K pages of total 12832)
	Node 4: Data usage is 2%(80 32K pages of total 3200)
	Node 4: Index usage is 0%(50 8K pages of total 12832)


>[注意]   
>因为仅仅在NDBCLUSTER[md]表上的DDL操作能同时执行，在执行下一个之前你必须等待每一个ALTER ONLINE TABLE ... REORGANIZE PARTITION[md]操作完成。   

对于创建的NDBCLUSTER表来说，在新的节点完全添加完成之后执行ALTER ONLINE TABLE ... REORGANIZE PARTITION语句，并不是必要的。往这些表中添加数据是自动在所有的数据节点中分配。然而，已经存在的节点的NDBCLUSTER表总是要优先于新的数据节点。直到使用ALTER ONLINE TABLE ... REORGANIZE PARTITION所有的表已经被重组完成之后，不管已经存在的或者新的数据都会被分配到新的节点中。   

**替代过程，没有滚动重启.**     通过配置额外的数据节点去绕开滚动重启是可能的，但是并不启动它们，当第一次启动集群时，我们假定，如以前，你希望在一个节点组去启动2个数据节点-节点1和2，接下来通过增加包含节点3和4的第二个节点组，集群扩充为4个数据节点：  


	[ndbd default]
	DataMemory = 100M
	IndexMemory = 100M
	NoOfReplicas = 2
	DataDir = /usr/local/mysql/var/mysql-cluster
	
	[ndbd]
	Id = 1
	HostName = 192.168.0.1
	
	[ndbd]
	Id = 2
	HostName = 192.168.0.2
	
	[ndbd]
	Id = 3
	HostName = 192.168.0.3
	Nodegroup = 65536   
	
	[ndbd]
	Id = 4
	HostName = 192.168.0.4
	Nodegroup = 65536   
	
	[mgm]
	HostName = 192.168.0.10
	Id = 10
	
	[api]
	Id=20
	HostName = 192.168.0.20
	
	[api]
	Id=21
	HostName = 192.168.0.21   


接下来的在线的数据节点(节点3和4)，可以使用NodeGroup = 65536 [2202]配置,在此组节点1和2如下所示被启动：  

	shell> ndbd -c 192.168.0.10 --initial


在通过设置StartNoNodeGroupTimeout [2222][md]数据节点配置参数来确定等待时间段后，就像你已经使用--nowait-nodes=3,4 [2331][md]启动过节点1和2一样，使用NodeGroup = 65536 [2202][md]配置的数据节点会被管理服务器一样来处理。默认，是15秒(15000毫秒)。   

> [注意]   
>对于集群中所有的数据节点，StartNoNodegroupTimeout [2222][md]应该是一样的。因此，比起个体的数据节点，你应该在文件config.ini的[ndbd default][md]段落设置好此值。

当你准备去添加第二个节点组时，你需要仅仅执行如下额外步骤：  

1 . 启动节点3和4，对于每一个新节点调用数据节点进程：  

	shell> ndbd -c 192.168.0.10 --initial

2 . 在管理客户端执行合适的CREATE NODEGROUP命令： 

	ndb_mgm> CREATE NODEGROUP 3,4

3 . 在mysql客户端[md]，对每一个已经存在的NDBCLUSTER表使用ALTER ONLINE TABLE ... REORGANIZE PARTITION和OPTIMIZE TABLE语句。(如同本节其他地方，直到新的节点完成，已经存在的MySQL集群表才能在数据再分配中使用新节点)   

